# Desafio Engenheiro de Dados
Meu desafio dentro desse case era desenvolver um ETL, que tem como objetivo tratar informaÃ§Ãµes de dados do Covid-19 (source ->Johns Hopkins atÃ© 2021).

## Resumo dos principais conceitos e technologias utilizadas.
* Pipeline criada na DAG do Airflow
* Airflow rodando localmente dentro de um container (docker)
* Processamento realizado via Spark tambÃ©m dentro do container (docker)
* FormataÃ§Ã£o especifica na hora de salvar os arquivos parquet, com objetivo de otimizaÃ§Ã£o e organizaÃ§Ã£o de arquivos, muito importante para FinOps e eficiencia dentro das clouds.
* Boas prÃ¡ticas em desenvolvimento de soluÃ§Ãµes, o cÃ³digo Ã© mantido em mÃ³dulos separados e bem documentados para facilitar a leitura e o debug.
* Brinquei um pouco com o conceito de dataquality na dag.
* Analise dos dados, R0 x MÃ©dia Movel


> ğŸ‘ï¸ Viz:
Segue abaixo uma imagem do painel de controle das runs de nossa dag.

![Sucess runs airflow](/_img/runs_airflow.png?raw=true)


#### Estrutura de diretÃ³rios e arquivos:

```
desafio-data-engineer/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ de_challenge_dag.py
â”‚   â”œâ”€â”€ sample/
â”‚   â”‚   â”œâ”€â”€ sample.py
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ resources/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_quality.py
â”‚   â”‚   â”‚   â”œâ”€â”€ process_tools.py
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_module.py
â”‚   â”‚   â”‚   â””â”€â”€  variables.py
â”‚   â”‚   â”œâ”€â”€ check_modules.py
â”‚   â”‚   â”œâ”€â”€ r0_modules.py
â”‚   â”‚   â”œâ”€â”€ refined_modules.py
â”‚   â”‚   â””â”€â”€ trusted_module.py
â”œâ”€â”€ datalake/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ refined/
â”‚   â”œâ”€â”€ research_r0/
â”‚   â””â”€â”€ trusted/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ simple_insights.ipynb
â””â”€â”€ trusted_check_notebook.ipynb

```

### DescriÃ§Ã£o da *DAG*

ğŸ‘ï¸ Viz:

![dag-de](/_img/dag_de.png?raw=true)

Na DAG acima, adicionei algumas etapas (tasks) adicionais que considero importantes para explorar em uma pipeline. O objetivo Ã© trazer esses conceitos de forma prÃ¡tica e simples, apenas para ilustrar sua aplicaÃ§Ã£o.

Abaixo vou destacar cada um e explicar seu motivo.

ğŸ‘ï¸ Viz:

![dag-de](/_img/dag_de_checks.png?raw=true)

De maneira resumida, o que trouxe aqui foram conceitos de data quality e a preparaÃ§Ã£o para possÃ­veis mudanÃ§as na source. Nas primeiras duas tasks, temos um exemplo de medir alguma mÃ©trica da source e, dado seu valor, a branch_task iria seguir o fluxo normal ou entrar em outro fluxo (para fazer algum outro tratamento ou aÃ§Ã£o, como enviar um e-mail ou mensagem no Discord).

Os outros checks sÃ£o mais para ilustrar possÃ­veis mediÃ§Ãµes nas camadas trusted e refined. Aqui, fiz muito uso do check_trusted para validar se os dados estavam corretos apÃ³s o join com a source (utilizei tambÃ©m o notebook trusted_check para visualizar a source).

Por Ãºltimo, adicionei uma task bem ao final, com o objetivo de somar algo (insights) Ã  camada refined. NÃ£o coloquei junto na mesma camada para nÃ£o atrapalhar a correÃ§Ã£o da task principal do case.

### Insights

Sobre a parte de analise de dados, fiz algo simples dentro do notebook simple_insights, onde busquei expor ambas as medidas calculadas na pipeline em forma de grÃ¡fico de linhas e trazer algumas conclusÃµes a partir dos dados.

### Desafios & dificuldades & opiniÃ£o do desafio


Aqui, creio que a maior dificuldade Ã© ter a vivÃªncia de trabalhar com dados pivoteados, algo que nÃ£o Ã© tÃ£o comum quanto o formato tabular. Mesmo jÃ¡ tendo feito a operaÃ§Ã£o de "despivotear", garantir que os dados estÃ£o corretos e fazer os checks na source pode ser desafiador, pois Ã© fÃ¡cil deixar passar algum detalhe despercebido.

No geral achei um desafio bem bacana de executar, gosto muito de aplicar soluÃ§Ãµes em docker no geral. Deu para ser de certa forma criativo e de adicionar algumas brincadeiras sem ser algo muito massante, acho isso Ã© importante para trazer aprendizados e leveza para dar gosto de fazer as atividades.

>ğŸ’» Note: 
Segue aqui tambÃ©m meu estudo de airflow-docker que fiz nesse ano, ficou bem legal tambÃ©m.
https://gabriel-philot.github.io/airflow_studies/ 

Aqui busco explorar alguns conceitos cores do airflow e tentar explicar ele meio que passo a passo em algumas versÃµes.